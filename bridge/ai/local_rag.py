import chromadb
from chromadb.utils import embedding_functions
from openai import OpenAI
import os
import glob
import pandas as pd
from pypdf import PdfReader
from docx import Document
import olefile
import zlib
import zipfile
import xml.etree.ElementTree as ET

# ìƒìœ„ ë””ë ‰í† ë¦¬(bridge)ë¥¼ sys.pathì— ì¶”ê°€í•˜ì—¬ config ì ‘ê·¼
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import config


# [ì„¤ì •] configì—ì„œ ê°€ì ¸ì˜¤ê¸°
BASE_URL = config.BASE_URL
API_KEY = config.API_KEY
MODEL_NAME = config.MODEL_NAME

# [ì¤‘ìš”] ê²½ë¡œ ì„¤ì •
CHROMA_DB_PATH = config.CHROMA_DB_PATH
DATA_DIR = config.DEFAULT_DATA_DIR

# --- íŒŒì¼ ì½ê¸° í•¨ìˆ˜ë“¤ ---

def read_text_file(path):
    with open(path, 'r', encoding='utf-8') as f:
        return f.read()

def read_pdf(path):
    """PDFë¥¼ í˜ì´ì§€ êµ¬ë¶„ê³¼ í•¨ê»˜ ì½ê¸°"""
    try:
        reader = PdfReader(path)
        text = ""
        for i, page in enumerate(reader.pages):
            page_text = page.extract_text()
            if page_text:
                text += f"\n[Page {i+1}]\n{page_text}\n"
        return text
    except Exception as e:
        return f"[PDF ì½ê¸° ì‹¤íŒ¨] {e}"

def read_docx(path):
    doc = Document(path)
    return "\n".join([para.text for para in doc.paragraphs])

def read_excel(path):
    # ëª¨ë“  ì‹œíŠ¸ë¥¼ ì½ì–´ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    dfs = pd.read_excel(path, sheet_name=None)
    text = ""
    for sheet_name, df in dfs.items():
        text += f"Sheet: {sheet_name}\n"
        text += df.to_string(index=False) + "\n\n"
    return text

def read_hwp(path):
    # HWP 5.0 í˜•ì‹ íŒŒì‹± (olefile + zlib)
    try:
        f = olefile.OleFileIO(path)
        dirs = f.listdir()
        
        # BodyText ì„¹ì…˜ ì°¾ê¸°
        body_sections = [d for d in dirs if d[0] == "BodyText"]
        text = ""
        
        for section in body_sections:
            stream = f.openstream(section)
            data = stream.read()
            
            # HWP íŒŒì¼ ìŠ¤íŠ¸ë¦¼ ì••ì¶• í•´ì œ (zlib)
            unpacked_data = zlib.decompress(data, -15)
            
            # UTF-16LE ì¸ì½”ë”© (HWP ë‚´ë¶€ í…ìŠ¤íŠ¸ ì¸ì½”ë”©)
            decoded = unpacked_data.decode('utf-16-le', errors='ignore')
            
            # ì¼ë°˜ í…ìŠ¤íŠ¸ë§Œ í•„í„°ë§
            clean_text = ""
            for char in decoded:
                if 32 <= ord(char) or ord(char) in [10, 13]: 
                    clean_text += char
            text += clean_text + "\n"
            
        return text
    except Exception as e:
        return f"[HWP ì½ê¸° ì‹¤íŒ¨] {e}"

def read_hwpx(path):
    # HWPXëŠ” ZIP í¬ë§·. Contents/section0.xml ë“±ì˜ XML íŒŒì‹±
    try:
        text = ""
        with zipfile.ZipFile(path, 'r') as z:
            # ì„¹ì…˜ íŒŒì¼ ì°¾ê¸°
            section_files = [n for n in z.namelist() if n.startswith("Contents/section") and n.endswith(".xml")]
            
            for section in section_files:
                xml_data = z.read(section)
                root = ET.fromstring(xml_data)
                # ëª¨ë“  í…ìŠ¤íŠ¸ ë…¸ë“œ ì¶”ì¶œ
                for node in root.iter():
                    if node.text:
                        text += node.text
                text += "\n"
        return text
    except Exception as e:
        return f"[HWPX ì½ê¸° ì‹¤íŒ¨] {e}"

# --- í…ìŠ¤íŠ¸ ì²­í‚¹ (Chunking) í•¨ìˆ˜ ---

def split_text(text, chunk_size=2500, overlap=400):
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ë©ì–´ë¦¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
    í‘œ ë°ì´í„° ë³´ì¡´ì„ ìœ„í•´ í° ë‹¨ìœ„ë¡œ ìë¦…ë‹ˆë‹¤.
    """
    if not text:
        return []
        
    chunks = []
    start = 0
    text_len = len(text)
    
    while start < text_len:
        end = start + chunk_size
        
        # í…ìŠ¤íŠ¸ ëì— ë„ë‹¬í•œ ê²½ìš°
        if end >= text_len:
            chunks.append(text[start:])
            break
            
        # ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„í• 
        last_newline = text.rfind('\n', start, end)
        if last_newline != -1 and last_newline > start + chunk_size * 0.5:
            end = last_newline + 1
        else:
            # ì¤„ë°”ê¿ˆì´ ì—†ìœ¼ë©´ ê³µë°± í™•ì¸
            last_space = text.rfind(' ', start, end)
            if last_space != -1 and last_space > start + chunk_size * 0.5:
                end = last_space + 1
        
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        # ê²¹ì¹˜ê²Œ ì´ë™ (overlap)
        start = end - overlap
        
    return chunks

def load_documents_from_folder(folder_path):
    docs = []
    metadatas = []
    ids = []
    
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
        print(f"[ì•Œë¦¼] '{folder_path}' í´ë”ê°€ ì—†ì–´ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        return [], [], []

    loaders = {
        ".txt": read_text_file,
        ".md": read_text_file,
        ".pdf": read_pdf,
        ".docx": read_docx,
        ".xlsx": read_excel,
        ".xls": read_excel,
        ".hwp": read_hwp,
        ".hwpx": read_hwpx
    }
    
    all_files = glob.glob(os.path.join(folder_path, "*.*"))
    print(f"[ì‹œìŠ¤í…œ] '{folder_path}' í´ë” ìŠ¤ìº” ì¤‘...")
    
    for i, file_path in enumerate(all_files):
        ext = os.path.splitext(file_path)[1].lower()
        if ext in loaders:
            try:
                print(f"   - ì½ëŠ” ì¤‘: {os.path.basename(file_path)}", end="", flush=True)
                content = loaders[ext](file_path)
                
                if content and len(content.strip()) > 10:
                    # [í•µì‹¬] ì½ì€ ë‚´ìš©ì„ ì²­í‚¹(Chunking)í•˜ì—¬ ì €ì¥
                    chunks = split_text(content, chunk_size=2500, overlap=400)
                    print(f" -> {len(chunks)}ê°œ ì¡°ê°ìœ¼ë¡œ ë¶„í•  ì €ì¥ [ì„±ê³µ]")
                    
                    for j, chunk in enumerate(chunks):
                        docs.append(chunk)
                        # ë©”íƒ€ë°ì´í„°ì— ì›ë³¸ íŒŒì¼ëª…ê³¼ ì¡°ê° ë²ˆí˜¸ ì €ì¥
                        metadatas.append({"source": os.path.basename(file_path), "type": ext, "chunk": j})
                        ids.append(f"{os.path.basename(file_path)}_chunk_{j}")
                else:
                    print(" [ê±´ë„ˆëœ€: ë‚´ìš© ì—†ìŒ]")
            except Exception as e:
                print(f" [ì‹¤íŒ¨: {e}]")
            
    return docs, metadatas, ids

def main():
    print("=" * 70)
    print("ğŸš€ vLLM RAG ì‹œìŠ¤í…œ (ë³´ì•ˆ ê°•í™” + í•œêµ­ì–´ ì •ë°€ ë¶„ì„)")
    print("=" * 70)
    print(f"ğŸ“ ì„œë²„: {BASE_URL}")
    print(f"ğŸ’¾ DB ê²½ë¡œ: {CHROMA_DB_PATH}")
    print(f"ğŸ“ ë¬¸ì„œ ê²½ë¡œ: {DATA_DIR}")
    print()

    # ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸
    print("[ì—°ê²° í…ŒìŠ¤íŠ¸] vLLM ì„œë²„ í™•ì¸ ì¤‘...", end="", flush=True)
    try:
        client = OpenAI(base_url=BASE_URL, api_key=API_KEY)
        models = client.models.list()
        print(" âœ… ì—°ê²° ì„±ê³µ!")
    except Exception as e:
        print(f" âŒ ì‹¤íŒ¨!\n")
        print("ğŸ”´ ì˜¤ë¥˜:", str(e))
        print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
        print("1. RunPod ì„œë²„ì—ì„œ vLLMì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸")
        print("2. SSH í„°ë„ë§ì´ ì¼œì ¸ ìˆëŠ”ì§€ í™•ì¸")
        return

    # [ì¶”ê°€] í•œêµ­ì–´ ì „ìš© ì„ë² ë”© ì—”ì§„
    print("\n[1/3] í•œêµ­ì–´ ì •ë°€ ê²€ìƒ‰ ì—”ì§„ ë¡œë“œ ì¤‘...")
    ko_embedding_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="jhgan/ko-sroberta-multitask"
    )

    print(f"[2/3] ChromaDB ë¡œë“œ ì¤‘... ({CHROMA_DB_PATH})")
    db_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    
    # [ìˆ˜ì •] í•œêµ­ì–´ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ëŠ” ì»¬ë ‰ì…˜ ìƒì„±
    collection = db_client.get_or_create_collection(
        "my_documents_ko",
        embedding_function=ko_embedding_ef
    )

    print("\në¬¸ì„œë¥¼ í™•ì¸í•©ë‹ˆë‹¤(ë³€ê²½ì‚¬í•­ ë°˜ì˜ì„ ìœ„í•´ ë§¤ë²ˆ ë‹¤ì‹œ ë¡œë“œ)...")
    docs, metas, ids = load_documents_from_folder(DATA_DIR)
    
    if docs:
        print(f"ì´ {len(docs)}ê°œì˜ í…ìŠ¤íŠ¸ ì¡°ê°(Chunk)ì„ DBì— ì €ì¥ ì¤‘...")
        collection.upsert(documents=docs, metadatas=metas, ids=ids)
        print("âœ… ì™„ë£Œ!")
    else:
        print("âš ï¸ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. my_data í´ë”ì— íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        return

    # [ìˆ˜ì •] ì˜ˆì‚° ë¶„ì„ íŠ¹í™” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt = """ë‹¹ì‹ ì€ ê³µê³µê¸°ê´€ ì˜ˆì‚° ì •ë°€ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[í•µì‹¬ ì›ì¹™]
1. í•œêµ­ì–´ë¡œ ì •ì¤‘í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
2. ì˜ˆì‚°Â·ìˆ˜ì¹˜ëŠ” ì²œ ë‹¨ìœ„ ì‰¼í‘œ í¬í•¨, 1ì› ë‹¨ìœ„ê¹Œì§€ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”. (ì˜ˆ: 20,377,728ì›)
3. í‘œ ë°ì´í„°ëŠ” 'í•­ëª©-ì‚°ì¶œì‹-ê¸ˆì•¡'ì˜ ë…¼ë¦¬ë¥¼ ëê¹Œì§€ ì¶”ì í•˜ì—¬ ì •í™•íˆ ì—°ê²°í•˜ì„¸ìš”.
4. ì„¸ë¶€ í•­ëª© í•©ê³„ì™€ ë¬¸ì„œì˜ 'ì´ê³„'ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ ë°˜ë“œì‹œ ê²€ì‚°í•˜ì„¸ìš”.
5. [ì°¸ê³  ì •ë³´]ì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
6. ë‹µë³€ ì‹œ ê°€ë…ì„±ì„ ìœ„í•´ í‘œ(Table) í˜•ì‹ì„ ì ê·¹ í™œìš©í•˜ì„¸ìš”.
7. ë‹µë³€ ëì—ëŠ” ì°¸ê³ í•œ íŒŒì¼ëª…ì„ ì–¸ê¸‰í•˜ì„¸ìš”."""

    print("\n[3/3] âœ… ì¤€ë¹„ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì¢…ë£Œ: quit)")
    print("\nğŸ’¡ ì¶”ì²œ ì§ˆë¬¸:")
    print("   - ëŒ€í•œë¯¼êµ­ì—­ì‚¬ë°•ë¬¼ê´€ ì˜ˆì‚°ì˜ ì„¸ë¶€ í•­ëª©ì„ í‘œë¡œ ì •ë¦¬í•´ì¤˜")
    print("   - ì¸ê±´ë¹„ ì‚°ì¶œ ë‚´ì—­ì„ ìƒì„¸íˆ ì•Œë ¤ì¤˜")
    print()

    while True:
        query = input("\nì§ˆë¬¸: ")
        if query.lower() in ["quit", "exit"]:
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        print("   ğŸ” ë¬¸ì„œ ì •ë°€ ê²€ìƒ‰ ì¤‘...", end="", flush=True)
        # 16k ì»¨í…ìŠ¤íŠ¸ì— ë§ì¶° 5ê°œë¡œ ì œí•œ
        results = collection.query(
            query_texts=[query],
            n_results=5 
        )
        
        retrieved_docs = results['documents'][0]
        retrieved_metas = results['metadatas'][0]
        
        if not retrieved_docs:
            print(" ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            continue

        # ê²€ìƒ‰ëœ ë‚´ìš© ì¡°í•© (ì¶œì²˜ í¬í•¨)
        context_text = ""
        sources = set()
        for doc, meta in zip(retrieved_docs, retrieved_metas):
            if meta is None:
                meta = {}
            source = meta.get('source', 'unknown')
            sources.add(source)
            context_text += f"[ì¶œì²˜: {source}]\n{doc}\n\n"
            
        print(f" ì™„ë£Œ! (ì°¸ê³  ë¬¸ì„œ: {len(retrieved_docs)}ê°œ ì¡°ê°)")

        augmented_prompt = f"""ì•„ë˜ ì •ë³´ë¥¼ ë©´ë°€íˆ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ì•„ì£¼ ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

[ì°¸ê³  ì •ë³´]
{context_text}

[ì§ˆë¬¸]
{query}"""

        print("   ğŸ¤– AI ë¶„ì„ ì¤‘:\n")
        
        try:
            stream = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": augmented_prompt}
                ],
                temperature=0.0,  # ì‚¬ì‹¤ ê¸°ë°˜ ê³ ì • (ì˜ˆì‚° ë¶„ì„ìš©)
                stream=True
            )

            for chunk in stream:
                if chunk.choices[0].delta.content:
                    print(chunk.choices[0].delta.content, end="", flush=True)
            
            print(f"\n\n   ğŸ“„ ì°¸ê³  íŒŒì¼: {', '.join(sources)}\n")
            
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜: {e}")
            print("ğŸ’¡ ì„œë²„ ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”. SSH í„°ë„ë§ì´ ì¼œì ¸ ìˆë‚˜ìš”?\n")

if __name__ == "__main__":
    main()
