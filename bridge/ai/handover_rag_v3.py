"""
ê³µê³µê¸°ê´€ ì¸ìˆ˜ì¸ê³„ RAG ì‹œìŠ¤í…œ v3 (LangGraph ê¸°ë°˜)
- êµ¬ì¡°í™”ëœ JSON ì‘ë‹µ
- í”„ë¡ íŠ¸ì—”ë“œ ëŒ€ì‹œë³´ë“œ ì—°ë™
- ê³µê³µê¸°ê´€ ì§€ì¹¨ ë‚´ì¥
"""

import json
import os
import glob
import re
from datetime import datetime
from typing import List, Dict, Any, Optional, TypedDict
from dataclasses import dataclass, asdict

import chromadb
from chromadb.utils import embedding_functions
from openai import OpenAI
from rank_bm25 import BM25Okapi
import numpy as np

# LangGraph
from langgraph.graph import StateGraph, END

# ============== ì„¤ì • (ì¤‘ì•™ config ì—°ë™) ==============
import config
# =================================


# ============== ê³µê³µê¸°ê´€ ê¸°ë³¸ ì§€ì¹¨ ==============
PUBLIC_INSTITUTION_GUIDELINES = """
## ğŸ“‹ ê³µê³µê¸°ê´€ ì—…ë¬´ì²˜ë¦¬ ê¸°ë³¸ ì§€ì¹¨

### 1. ë¬¸ì„œ ë¶„ë¥˜ ì²´ê³„
- **ê¸°ì•ˆë¬¸**: ì—…ë¬´ ì‹œì‘, ì˜ˆì‚° ìš”ì²­, ê³„íš ìˆ˜ë¦½
- **ê³„ì•½ì„œ**: ìš©ì—­ê³„ì•½, ë¬¼í’ˆêµ¬ë§¤, ì‹œì„¤ê³µì‚¬
- **í’ˆì˜ì„œ**: ì§€ì¶œ ìŠ¹ì¸ ìš”ì²­
- **ê²°ì¬ë¬¸ì„œ**: ìµœì¢… ìŠ¹ì¸ ë¬¸ì„œ
- **ê²€ìˆ˜ì¡°ì„œ**: ë‚©í’ˆ/ìš©ì—­ ì™„ë£Œ í™•ì¸
- **ì •ì‚°ì„œ**: ì‚¬ì—… ì¢…ë£Œ í›„ ì •ì‚°

### 2. ì˜ˆì‚° ì§‘í–‰ í”„ë¡œì„¸ìŠ¤
1) ê¸°ë³¸ê³„íš ìˆ˜ë¦½ â†’ ì˜ˆì‚° í™•ë³´
2) ì‚¬ì—…ì ì„ ì • â†’ ê³„ì•½ ì²´ê²°
3) ì‚¬ì—… ìˆ˜í–‰ â†’ ì¤‘ê°„ì ê²€
4) ê²€ìˆ˜ â†’ ëŒ€ê¸ˆ ì§€ê¸‰
5) ì •ì‚° â†’ ì‚¬ì—… ì¢…ë£Œ

### 3. ì£¼ìš” ë²•ë ¹ ë° ê·œì •
- ã€Œêµ­ê°€ì¬ì •ë²•ã€: ì˜ˆì‚° í¸ì„± ë° ì§‘í–‰
- ã€Œêµ­ê°€ë¥¼ ë‹¹ì‚¬ìë¡œ í•˜ëŠ” ê³„ì•½ì— ê´€í•œ ë²•ë¥ ã€: ê³„ì•½ ì ˆì°¨
- ã€Œê³µê³µê¸°ê´€ì˜ ìš´ì˜ì— ê´€í•œ ë²•ë¥ ã€: ê¸°ê´€ ìš´ì˜
- ã€Œì •ë¶€ì—…ë¬´í‰ê°€ ê¸°ë³¸ë²•ã€: ì„±ê³¼ ì¸¡ì •

### 4. ê¸ˆì•¡ í‘œê¸° ì›ì¹™
- ì²œ ë‹¨ìœ„ ì‰¼í‘œ í•„ìˆ˜ (ì˜ˆ: 20,377,728ì›)
- ë¶€ê°€ì„¸ í¬í•¨/ë³„ë„ ëª…ì‹œ
- ì˜ˆì‚°ê³¼ ì‹¤ì§‘í–‰ì•¡ êµ¬ë¶„

### 5. ì£¼ì˜ì‚¬í•­ (Issues ë¶„ë¥˜)
- ğŸ”´ critical: ë²•ë ¹ ìœ„ë°˜, ê¸ˆì•¡ ì˜¤ë¥˜
- ğŸŸ¡ warn: ì ˆì°¨ ëˆ„ë½, ì„œë¥˜ ë¯¸ë¹„
- ğŸ”µ info: ì°¸ê³ ì‚¬í•­, ê¶Œê³ ì‚¬í•­
"""


# ============== ë°ì´í„° ëª¨ë¸ ==============
class ProjectState(TypedDict):
    """LangGraph ìƒíƒœ ì •ì˜"""
    query: str
    query_type: str
    files: List[Dict]
    retrieved_docs: List[Dict]
    timeline: Dict
    issues: List[Dict]
    summary: Dict
    response_json: Dict
    final_answer: str


@dataclass
class TimelineEvent:
    date: str
    label: str
    description: str
    phaseId: str
    fileId: str
    amount: Optional[int] = None
    highlight: bool = False


@dataclass
class TimelinePhase:
    id: str
    name: str
    color: str


@dataclass 
class Issue:
    level: str  # critical, warn, info
    title: str
    description: str
    suggestion: str
    relatedFileIds: List[str]


# ============== ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ ==============
class QueryClassifier:
    QUERY_TYPES = {
        "budget": ["ì˜ˆì‚°", "ê¸ˆì•¡", "ë¹„ìš©", "ì›", "ê²°ì‚°", "ì„¸ì¶œ", "ì„¸ì…", "ì‚°ì¶œ", "ì •ì‚°"],
        "contract": ["ê³„ì•½", "ìš©ì—­", "ì…ì°°", "ë‚™ì°°", "ìˆ˜ì˜ê³„ì•½", "ì—…ì²´", "ì‚¬ì—…ì"],
        "process": ["ì ˆì°¨", "í”„ë¡œì„¸ìŠ¤", "ìˆœì„œ", "ë‹¨ê³„", "ë°©ë²•", "ì§„í–‰"],
        "regulation": ["ê·œì •", "ì§€ì¹¨", "ë²•ë ¹", "ì¡°í•­", "ê¸°ì¤€"],
        "timeline": ["ì¼ì •", "ê¸°ê°„", "ì–¸ì œ", "ë‚ ì§œ", "ì—°í˜", "ì´ë ¥"],
        "organization": ["ë‹´ë‹¹ì", "ë¶€ì„œ", "ì¡°ì§", "ë‹´ë‹¹", "ì—°ë½ì²˜"]
    }
    
    @classmethod
    def classify(cls, query: str) -> tuple:
        scores = {qtype: sum(1 for kw in keywords if kw in query) 
                  for qtype, keywords in cls.QUERY_TYPES.items()}
        if max(scores.values()) == 0:
            return "general", 0.0
        best_type = max(scores, key=scores.get)
        return best_type, scores[best_type] / len(cls.QUERY_TYPES[best_type])


# ============== í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ==============
class HybridSearcher:
    def __init__(self, collection):
        self.collection = collection
        self.documents = []
        self.doc_ids = []
        self.metadatas = []
        self.bm25 = None
        self._build_index()
    
    def _tokenize(self, text: str) -> List[str]:
        text = re.sub(r'[^\w\s]', ' ', text)
        return [t for t in text.split() if len(t) > 1]
    
    def _build_index(self):
        all_docs = self.collection.get(include=["documents", "metadatas"])
        if not all_docs['documents']:
            return
        self.documents = all_docs['documents']
        self.doc_ids = all_docs['ids']
        self.metadatas = all_docs['metadatas']
        tokenized = [self._tokenize(doc) for doc in self.documents]
        self.bm25 = BM25Okapi(tokenized)
    
    def search(self, query: str, n_results: int = 5) -> List[Dict]:
        # ë²¡í„° ê²€ìƒ‰
        vector_results = self.collection.query(query_texts=[query], n_results=n_results * 2)
        
        # BM25 ê²€ìƒ‰
        bm25_scores = {}
        if self.bm25:
            scores = self.bm25.get_scores(self._tokenize(query))
            for idx in np.argsort(scores)[::-1][:n_results * 2]:
                if scores[idx] > 0:
                    bm25_scores[self.doc_ids[idx]] = scores[idx]
        
        # ì ìˆ˜ ë³‘í•©
        combined = {}
        for i, doc_id in enumerate(vector_results['ids'][0]):
            combined[doc_id] = 0.7 * (1 / (1 + i))
        
        if bm25_scores:
            max_bm25 = max(bm25_scores.values())
            for doc_id, score in bm25_scores.items():
                combined[doc_id] = combined.get(doc_id, 0) + 0.3 * (score / max_bm25)
        
        # ìƒìœ„ ê²°ê³¼ ë°˜í™˜
        sorted_ids = sorted(combined.keys(), key=lambda x: combined[x], reverse=True)[:n_results]
        
        results = []
        for doc_id in sorted_ids:
            idx = self.doc_ids.index(doc_id)
            results.append({
                "id": doc_id,
                "content": self.documents[idx],
                "metadata": self.metadatas[idx]
            })
        return results


# ============== RAG ì—”ì§„ ==============
class HandoverRAGEngine:
    def __init__(self, base_url: Optional[str] = None):
        import httpx
        self.base_url = base_url or config.LLM_API_URL
        # SSL ê²€ì¦ ë¬´ì‹œ (RunPod í”„ë¡ì‹œ ëŒ€ì‘)
        http_client = httpx.Client(verify=False)
        self.client = OpenAI(base_url=self.base_url, api_key=config.API_KEY, http_client=http_client)
        self.collection = None
        self.searcher = None
        self.graph = None
        
    def setup(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        print("=" * 70)
        print("ğŸ›ï¸ ê³µê³µê¸°ê´€ ì¸ìˆ˜ì¸ê³„ RAG ì‹œìŠ¤í…œ v3 (LangGraph)")
        print("=" * 70)
        
        # ì„œë²„ ì—°ê²° í™•ì¸
        print("[1/4] vLLM ì„œë²„ ì—°ê²° í™•ì¸...", end="", flush=True)
        try:
            self.client.models.list()
            print(" âœ…")
        except Exception as e:
            print(f" âŒ\n{e}")
            return False
        
        # ChromaDB ì„¤ì •
        print("[2/4] ChromaDB ì´ˆê¸°í™”...", end="", flush=True)
        ko_embedding = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="jhgan/ko-sroberta-multitask"
        )
        db_client = chromadb.PersistentClient(path=config.CHROMA_DB_PATH)
        self.collection = db_client.get_or_create_collection(
            "handover_v3", embedding_function=ko_embedding
        )
        print(" âœ…")
        
        # ë¬¸ì„œ ë¡œë“œ
        print("[3/4] ë¬¸ì„œ ë¡œë“œ ì¤‘...")
        # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        print("[3/4] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”...", end="", flush=True)
        self.searcher = HybridSearcher(self.collection)
        print(" âœ…")
        
        # LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±
        self._build_graph()
        
        print("\nâœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
        return True
    
    def load_directory(self, path: str):
        """
        [ë™ì  ë¡œë“œ] ì‚¬ìš©ìê°€ ì„ íƒí•œ í´ë”ì˜ ë¬¸ì„œë¥¼ ìƒ‰ì¸í•©ë‹ˆë‹¤.
        ê°€ì¥ ìµœì‹  ì •ë³´ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ ê¸°ì¡´ DBë¥¼ ë°€ê³  ìƒˆë¡œ ë§Œë“­ë‹ˆë‹¤.
        """
        print(f"\nğŸ“‚ AI ì—”ì§„ ìƒ‰ì¸ ì—…ë°ì´íŠ¸: {path}")
        
        from local_rag import read_pdf, read_docx, read_excel, read_hwp, read_hwpx, read_text_file, split_text
        
        loaders = {
            ".pdf": read_pdf, ".docx": read_docx, ".xlsx": read_excel,
            ".xls": read_excel, ".hwp": read_hwp, ".hwpx": read_hwpx,
            ".txt": read_text_file, ".md": read_text_file
        }
        
        if not os.path.exists(path):
            return
        
        # í•˜ìœ„ ë””ë ‰í† ë¦¬ê¹Œì§€ ì¬ê·€ ê²€ìƒ‰
        all_files = glob.glob(os.path.join(path, "**", "*.*"), recursive=True)
        docs, metas, ids = [], [], []
        
        for file_path in all_files:
            ext = os.path.splitext(file_path)[1].lower()
            if ext not in loaders: continue
            
            try:
                content = loaders[ext](file_path)
                if content and len(content.strip()) > 10:
                    chunks = split_text(content, chunk_size=1500, overlap=300)
                    for j, chunk in enumerate(chunks):
                        docs.append(chunk)
                        metas.append({
                            "fileId": f"file-{len(docs)}",
                            "source": os.path.basename(file_path),
                            "docType": self._infer_doc_type(file_path, chunk),
                            "chunk": j,
                            "date": self._extract_date(chunk)
                        })
                        ids.append(f"{os.path.basename(file_path)}_{j}")
            except Exception as e:
                print(f"   ! AI ìƒ‰ì¸ ì˜¤ë¥˜ ({os.path.basename(file_path)}): {e}")

        # ê¸°ì¡´ ë°ì´í„° ì´ˆê¸°í™” (í•­ìƒ ìµœì‹  ìƒíƒœ ë³´ì¥)
        try:
            self.collection.delete(where={})
        except:
            pass

        if docs:
            self.collection.upsert(documents=docs, metadatas=metas, ids=ids)
            print(f"   ğŸ“š AI ì—”ì§„ ìƒ‰ì¸ ì™„ë£Œ (ì´ {len(docs)}ê°œ ì²­í¬)")
            self.searcher = HybridSearcher(self.collection)
    
    def _infer_doc_type(self, path: str, content: str) -> str:
        name = os.path.basename(path).lower()
        if any(kw in name for kw in ["ì˜ˆì‚°", "ê²°ì‚°", "ì‚°ì¶œ"]):
            return "budget"
        elif any(kw in name for kw in ["ê³„ì•½", "ìš©ì—­"]):
            return "contract"
        elif any(kw in name for kw in ["ê·œì •", "ì§€ì¹¨"]):
            return "regulation"
        elif "ì›" in content and re.search(r'\d{1,3}(,\d{3})+', content):
            return "budget"
        return "general"
    
    def _extract_date(self, content: str) -> str:
        match = re.search(r'(\d{4})[-./](\d{1,2})[-./](\d{1,2})', content)
        if match:
            return f"{match.group(1)}-{match.group(2).zfill(2)}-{match.group(3).zfill(2)}"
        return ""
    
    def _build_graph(self):
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±"""
        
        def classify_query(state: ProjectState) -> ProjectState:
            """Step 1: ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜"""
            query_type, _ = QueryClassifier.classify(state["query"])
            state["query_type"] = query_type
            return state
        
        def retrieve_documents(state: ProjectState) -> ProjectState:
            """Step 2: ë¬¸ì„œ ê²€ìƒ‰"""
            results = self.searcher.search(state["query"], n_results=5)
            state["retrieved_docs"] = results
            return state
        
        def build_structure(state: ProjectState) -> ProjectState:
            """Step 3: êµ¬ì¡°í™”ëœ ë°ì´í„° ìƒì„±"""
            docs = state["retrieved_docs"]
            
            # íŒŒì¼ ëª©ë¡ êµ¬ì„±
            files = []
            seen_sources = set()
            for doc in docs:
                source = doc["metadata"].get("source", "unknown")
                if source not in seen_sources:
                    seen_sources.add(source)
                    files.append({
                        "id": doc["metadata"].get("fileId", doc["id"]),
                        "name": source,
                        "docType": doc["metadata"].get("docType", "general"),
                        "date": doc["metadata"].get("date", ""),
                        "summary": doc["content"][:200] + "..."
                    })
            state["files"] = files
            
            # íƒ€ì„ë¼ì¸ êµ¬ì„±
            phases = [
                {"id": "plan", "name": "ê¸°íš", "color": "#3b82f6"},
                {"id": "contract", "name": "ê³„ì•½", "color": "#8b5cf6"},
                {"id": "execute", "name": "ì§‘í–‰", "color": "#10b981"},
                {"id": "close", "name": "ì •ì‚°", "color": "#f59e0b"}
            ]
            
            events = []
            for file in files:
                if file["date"]:
                    phase_id = "plan" if file["docType"] == "budget" else \
                               "contract" if file["docType"] == "contract" else "execute"
                    events.append({
                        "date": file["date"],
                        "label": file["name"],
                        "description": file["summary"][:100],
                        "phaseId": phase_id,
                        "fileId": file["id"],
                        "highlight": False
                    })
            
            state["timeline"] = {"phases": phases, "events": sorted(events, key=lambda x: x["date"])}
            
            # ì´ìŠˆ ë¶„ì„ (ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹±)
            issues = []
            for doc in docs:
                content = doc["content"]
                if "ë³€ê²½" in content and "ê³„ì•½" in content:
                    issues.append({
                        "level": "warn",
                        "title": "ê³„ì•½ ë³€ê²½ ê°ì§€",
                        "description": "ê³„ì•½ ë³€ê²½ ê´€ë ¨ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                        "suggestion": "ë³€ê²½ê³„ì•½ì„œ ë° ì‚¬ìœ ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                        "relatedFileIds": [doc["metadata"].get("fileId", doc["id"])]
                    })
            state["issues"] = issues
            
            return state
        
        def generate_response(state: ProjectState) -> ProjectState:
            """Step 4: AI ì‘ë‹µ ìƒì„± (JSON í˜•ì‹)"""
            context = "\n\n".join([
                f"[{doc['metadata'].get('source', 'unknown')}]\n{doc['content']}"
                for doc in state["retrieved_docs"]
            ])
            
            system_prompt = f"""ë‹¹ì‹ ì€ ê³µê³µê¸°ê´€ ì—…ë¬´ ì¸ìˆ˜ì¸ê³„ AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

{PUBLIC_INSTITUTION_GUIDELINES}

## ì‘ë‹µ ê·œì¹™
1. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
2. ê¸ˆì•¡ì€ ì²œ ë‹¨ìœ„ ì‰¼í‘œ í¬í•¨ (ì˜ˆ: 20,377,728ì›)
3. í‘œ ë°ì´í„°ëŠ” items ë°°ì—´ë¡œ êµ¬ì¡°í™”
4. ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ì•ˆë‚´

## JSON ì‘ë‹µ í˜•ì‹
```json
{{
  "greeting": "ì•ˆë…•í•˜ì„¸ìš”! [ì§ˆë¬¸ ì£¼ì œ]ì— ëŒ€í•´ ì•ˆë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
  "summary": {{
    "title": "ì œëª©",
    "totalAmount": 0,
    "period": "ê¸°ê°„",
    "keyPoints": ["í•µì‹¬ í¬ì¸íŠ¸ 1", "í•µì‹¬ í¬ì¸íŠ¸ 2"]
  }},
  "details": {{
    "description": "ìƒì„¸ ì„¤ëª…",
    "items": [
      {{"name": "í•­ëª©ëª…", "calculation": "ì‚°ì¶œì‹", "amount": 0}}
    ]
  }},
  "regulations": ["ê´€ë ¨ ê·œì • 1", "ê´€ë ¨ ê·œì • 2"],
  "tips": ["ì¸ìˆ˜ì¸ê³„ íŒ 1", "ì‹¤ë¬´ ë…¸í•˜ìš°"],
  "closing": "ì¶”ê°€ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!"
}}
```"""

            user_prompt = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
{context}

[ì§ˆë¬¸]
{state["query"]}

ë°˜ë“œì‹œ ìœ„ JSON í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”."""

            try:
                response = self.client.chat.completions.create(
                    model=config.MODEL_NAME,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.0
                )
                
                answer = response.choices[0].message.content
                
                # JSON ì¶”ì¶œ ì‹œë„
                json_match = re.search(r'\{[\s\S]*\}', answer)
                if json_match:
                    try:
                        response_json = json.loads(json_match.group())
                        state["response_json"] = response_json
                    except:
                        state["response_json"] = {"raw": answer}
                else:
                    state["response_json"] = {"raw": answer}
                
                state["final_answer"] = answer
                
            except Exception as e:
                state["final_answer"] = f"ì˜¤ë¥˜ ë°œìƒ: {e}"
                state["response_json"] = {"error": str(e)}
            
            return state
        
        # ê·¸ë˜í”„ êµ¬ì„±
        workflow = StateGraph(ProjectState)
        
        workflow.add_node("classify", classify_query)
        workflow.add_node("retrieve", retrieve_documents)
        workflow.add_node("structure", build_structure)
        workflow.add_node("generate", generate_response)
        
        workflow.set_entry_point("classify")
        workflow.add_edge("classify", "retrieve")
        workflow.add_edge("retrieve", "structure")
        workflow.add_edge("structure", "generate")
        workflow.add_edge("generate", END)
        
        self.graph = workflow.compile()
    
    def ask(self, query: str) -> Dict:
        """ì§ˆë¬¸ ì²˜ë¦¬ ë° êµ¬ì¡°í™”ëœ ì‘ë‹µ ë°˜í™˜"""
        print(f"\nğŸ“ ì§ˆë¬¸: {query}")
        print("=" * 50)
        
        initial_state = {
            "query": query,
            "query_type": "",
            "files": [],
            "retrieved_docs": [],
            "timeline": {},
            "issues": [],
            "summary": {},
            "response_json": {},
            "final_answer": ""
        }
        
        print("ğŸ”„ ì²˜ë¦¬ ì¤‘...")
        result = self.graph.invoke(initial_state)
        
        # ìµœì¢… ì‘ë‹µ êµ¬ì„±
        final_response = {
            "project": {
                "id": f"proj-{datetime.now().strftime('%Y%m%d%H%M%S')}",
                "name": query[:30],
                "fileCount": len(result["files"]),
                "files": result["files"]
            },
            "summary": {
                "timeline": result["timeline"],
                "issues": result["issues"]
            },
            "answer": result["response_json"],
            "sources": [f["name"] for f in result["files"]]
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 50)
        print("ğŸ“Š ì§ˆë¬¸ ìœ í˜•:", result["query_type"])
        print("ğŸ“š ì°¸ê³  ë¬¸ì„œ:", len(result["files"]), "ê°œ")
        print("âš ï¸  ì´ìŠˆ:", len(result["issues"]), "ê°œ")
        print("=" * 50)
        
        # AI ì‘ë‹µ ì¶œë ¥
        if "greeting" in result["response_json"]:
            print(f"\nğŸ¤– {result['response_json']['greeting']}")
            
            if "summary" in result["response_json"]:
                summary = result["response_json"]["summary"]
                print(f"\nğŸ“‹ {summary.get('title', 'ìš”ì•½')}")
                if "totalAmount" in summary:
                    print(f"   ğŸ’° ì´ì•¡: {summary['totalAmount']:,}ì›" if isinstance(summary['totalAmount'], int) else f"   ğŸ’° ì´ì•¡: {summary['totalAmount']}")
                if "keyPoints" in summary:
                    for point in summary["keyPoints"]:
                        print(f"   â€¢ {point}")
            
            if "details" in result["response_json"] and "items" in result["response_json"]["details"]:
                print("\nğŸ“Š ì„¸ë¶€ í•­ëª©:")
                print("-" * 60)
                for item in result["response_json"]["details"]["items"]:
                    print(f"   {item.get('name', '')} | {item.get('calculation', '')} | {item.get('amount', 0):,}ì›" if isinstance(item.get('amount', 0), int) else f"   {item}")
                print("-" * 60)
            
            if "tips" in result["response_json"]:
                print("\nğŸ’¡ ì¸ìˆ˜ì¸ê³„ íŒ:")
                for tip in result["response_json"]["tips"]:
                    print(f"   â€¢ {tip}")
            
            if "closing" in result["response_json"]:
                print(f"\n{result['response_json']['closing']}")
        else:
            print("\nğŸ¤– AI ì‘ë‹µ:")
            print(result["final_answer"])
        
        print(f"\nğŸ“„ ì¶œì²˜: {', '.join(final_response['sources'])}")
        
        return final_response
    
    def run(self):
        """ëŒ€í™”í˜• ì‹¤í–‰"""
        print("\n" + "=" * 70)
        print("ğŸ’¬ ì§ˆë¬¸ ì˜ˆì‹œ:")
        print("   - 2026ë…„ ì˜ˆì‚° ì„¸ë¶€ í•­ëª©ì„ ì •ë¦¬í•´ì¤˜")
        print("   - ê³„ì•½ ì§„í–‰ ì ˆì°¨ë¥¼ ì•Œë ¤ì¤˜")
        print("   - ë‹´ë‹¹ì ì—…ë¬´ ë¶„ì¥ì„ ì„¤ëª…í•´ì¤˜")
        print("=" * 70)
        print("(ì¢…ë£Œ: quit)\n")
        
        while True:
            query = input("\nì§ˆë¬¸: ")
            if query.lower() in ["quit", "exit", "ì¢…ë£Œ"]:
                print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not query.strip():
                continue
            
            result = self.ask(query)
            
            # JSON ìë™ ì €ì¥
            filename = f"response_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"\nâœ… JSON ì €ì¥ë¨: {filename}")


def main():
    # ì˜ì¡´ì„± í™•ì¸
    try:
        from langgraph.graph import StateGraph
    except ImportError:
        print("âš ï¸ langgraph ì„¤ì¹˜ í•„ìš”: pip install langgraph")
        return
    
    try:
        from rank_bm25 import BM25Okapi
    except ImportError:
        print("âš ï¸ rank_bm25 ì„¤ì¹˜ í•„ìš”: pip install rank-bm25")
        return
    
    engine = HandoverRAGEngine()
    if engine.setup():
        engine.run()


if __name__ == "__main__":
    main()
