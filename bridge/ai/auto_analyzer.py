"""
ê³µê³µê¸°ê´€ ì¸ìˆ˜ì¸ê³„ ë¬¸ì„œ ìë™ ë¶„ì„ ì‹œìŠ¤í…œ
- íŒŒì¼ ì—…ë¡œë“œ â†’ ìë™ JSON ìƒì„±
- í”„ë¡ íŠ¸ì—”ë“œ ëŒ€ì‹œë³´ë“œ ì—°ë™ ìŠ¤í™ ì¤€ìˆ˜
"""

import json
import os
import glob
import re
from datetime import datetime
from typing import List, Dict, Optional

import chromadb
from chromadb.utils import embedding_functions
from openai import OpenAI
from rank_bm25 import BM25Okapi
import numpy as np

# ============== ì„¤ì • ==============
BASE_URL = "http://localhost:8000/v1"
API_KEY = "EMPTY"
MODEL_NAME = "mistralai/Mistral-Nemo-Instruct-2407"

CHROMA_DB_PATH = "./chroma_db_auto"
DATA_DIR = "./my_data"
OUTPUT_DIR = "./outputs"
# =================================


# ============== ê³µê³µê¸°ê´€ ê¸°ë³¸ ì§€ì¹¨ ==============
PUBLIC_INSTITUTION_GUIDELINES = """
## ğŸ“‹ ê³µê³µê¸°ê´€ ì—…ë¬´ì²˜ë¦¬ ê¸°ë³¸ ì§€ì¹¨

### 1. ë¬¸ì„œ ë¶„ë¥˜ ì²´ê³„
- **ê¸°ì•ˆë¬¸**: ì—…ë¬´ ì‹œì‘, ì˜ˆì‚° ìš”ì²­, ê³„íš ìˆ˜ë¦½
- **ê³„ì•½ì„œ**: ìš©ì—­ê³„ì•½, ë¬¼í’ˆêµ¬ë§¤, ì‹œì„¤ê³µì‚¬
- **í’ˆì˜ì„œ**: ì§€ì¶œ ìŠ¹ì¸ ìš”ì²­
- **ê²°ì¬ë¬¸ì„œ**: ìµœì¢… ìŠ¹ì¸ ë¬¸ì„œ
- **ê²€ìˆ˜ì¡°ì„œ**: ë‚©í’ˆ/ìš©ì—­ ì™„ë£Œ í™•ì¸
- **ì •ì‚°ì„œ**: ì‚¬ì—… ì¢…ë£Œ í›„ ì •ì‚°

### 2. ì˜ˆì‚° ì§‘í–‰ í”„ë¡œì„¸ìŠ¤
1) ê¸°íš: ê¸°ë³¸ê³„íš ìˆ˜ë¦½ â†’ ì˜ˆì‚° í™•ë³´
2) ê³„ì•½: ì‚¬ì—…ì ì„ ì • â†’ ê³„ì•½ ì²´ê²°
3) ì§‘í–‰: ì‚¬ì—… ìˆ˜í–‰ â†’ ì¤‘ê°„ì ê²€
4) ì •ì‚°: ê²€ìˆ˜ â†’ ëŒ€ê¸ˆ ì§€ê¸‰ â†’ ì‚¬ì—… ì¢…ë£Œ

### 3. ì£¼ìš” ë²•ë ¹ ë° ê·œì •
- ã€Œêµ­ê°€ì¬ì •ë²•ã€: ì˜ˆì‚° í¸ì„± ë° ì§‘í–‰
- ã€Œêµ­ê°€ë¥¼ ë‹¹ì‚¬ìë¡œ í•˜ëŠ” ê³„ì•½ì— ê´€í•œ ë²•ë¥ ã€
- ã€Œê³µê³µê¸°ê´€ì˜ ìš´ì˜ì— ê´€í•œ ë²•ë¥ ã€
- ã€Œì •ë¶€ì—…ë¬´í‰ê°€ ê¸°ë³¸ë²•ã€

### 4. ì´ìŠˆ ë¶„ë¥˜ ê¸°ì¤€
- ğŸ”´ critical: ë²•ë ¹ ìœ„ë°˜, ê¸ˆì•¡ ì˜¤ë¥˜, ê³„ì•½ ìœ„ë°˜
- ğŸŸ¡ warn: ì ˆì°¨ ëˆ„ë½, ì„œë¥˜ ë¯¸ë¹„, ë³€ê²½ì‚¬í•­
- ğŸ”µ info: ì°¸ê³ ì‚¬í•­, ê¶Œê³ ì‚¬í•­
"""


class DocumentAnalyzer:
    """ë¬¸ì„œ ìë™ ë¶„ì„ê¸°"""
    
    # ë¬¸ì„œ ìœ í˜•ë³„ í˜ì´ì¦ˆ ë§¤í•‘
    DOC_TYPE_TO_PHASE = {
        "ê¸°ì•ˆ": "plan",
        "ê³„íš": "plan", 
        "ì˜ˆì‚°": "plan",
        "ê³„ì•½": "contract",
        "ìš©ì—­": "contract",
        "ì…ì°°": "contract",
        "ê²€ìˆ˜": "execute",
        "ë‚©í’ˆ": "execute",
        "ì§‘í–‰": "execute",
        "ì •ì‚°": "close",
        "ê²°ì‚°": "close"
    }
    
    def __init__(self, base_url: Optional[str] = None):
        self.base_url = base_url or BASE_URL
        self.client = OpenAI(base_url=self.base_url, api_key=API_KEY)
        self.collection = None
        self.files_data = []
        
    def setup(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        print("=" * 70)
        print("ğŸ›ï¸ ê³µê³µê¸°ê´€ ì¸ìˆ˜ì¸ê³„ ìë™ ë¶„ì„ ì‹œìŠ¤í…œ")
        print("=" * 70)
        
        # ì„œë²„ ì—°ê²° í™•ì¸
        print("[1/3] vLLM ì„œë²„ ì—°ê²° í™•ì¸...", end="", flush=True)
        try:
            self.client.models.list()
            print(" âœ…")
        except Exception as e:
            print(f" âŒ\n{e}")
            return False
        
        # ChromaDB ì„¤ì •
        print("[2/3] ChromaDB ì´ˆê¸°í™”...", end="", flush=True)
        ko_embedding = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="jhgan/ko-sroberta-multitask"
        )
        db_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
        self.collection = db_client.get_or_create_collection(
            "auto_analysis", embedding_function=ko_embedding
        )
        print(" âœ…")
        
        # ì¶œë ¥ í´ë” ìƒì„±
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        
        print("[3/3] ì¤€ë¹„ ì™„ë£Œ!")
        return True
    
    def analyze_all(self) -> Dict:
        """ëª¨ë“  ë¬¸ì„œ ìë™ ë¶„ì„í•˜ì—¬ JSON ìƒì„±"""
        print("\nğŸ“‚ ë¬¸ì„œ ë¶„ì„ ì‹œì‘...")
        
        # 1. ë¬¸ì„œ ë¡œë“œ ë° íŒŒì‹±
        self._load_and_parse_documents()
        
        if not self.files_data:
            print("âš ï¸ ë¶„ì„í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. my_data/ í´ë”ì— íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
            return {}
        
        # 2. êµ¬ì¡°í™”ëœ ë°ì´í„° ìƒì„±
        project_data = self._build_project_structure()
        
        # 3. JSON ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{OUTPUT_DIR}/analysis_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(project_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ! JSON ì €ì¥ë¨: {filename}")
        
        return project_data
    
    def _load_and_parse_documents(self):
        """ë¬¸ì„œ ë¡œë“œ ë° íŒŒì‹±"""
        from local_rag import (
            read_pdf, read_docx, read_excel, read_hwp, read_hwpx,
            read_text_file, split_text
        )
        
        loaders = {
            ".pdf": read_pdf, ".docx": read_docx, ".xlsx": read_excel,
            ".xls": read_excel, ".hwp": read_hwp, ".hwpx": read_hwpx,
            ".txt": read_text_file, ".md": read_text_file
        }
        
        if not os.path.exists(DATA_DIR):
            os.makedirs(DATA_DIR)
            return
        
        all_files = glob.glob(os.path.join(DATA_DIR, "*.*"))
        self.files_data = []
        
        for idx, file_path in enumerate(all_files):
            ext = os.path.splitext(file_path)[1].lower()
            if ext not in loaders:
                continue
            
            try:
                content = loaders[ext](file_path)
                if not content or len(content.strip()) < 10:
                    continue
                
                filename = os.path.basename(file_path)
                file_id = f"file-{idx + 1}"
                
                # ë¬¸ì„œ ë¶„ì„
                doc_info = self._analyze_single_document(
                    file_id, filename, content
                )
                
                self.files_data.append(doc_info)
                print(f"   âœ… {filename}")
                
                # ChromaDBì— ì €ì¥ (ê²€ìƒ‰ìš©)
                chunks = split_text(content, chunk_size=1500, overlap=300)
                for j, chunk in enumerate(chunks):
                    self.collection.upsert(
                        documents=[chunk],
                        metadatas=[{"fileId": file_id, "source": filename}],
                        ids=[f"{filename}_chunk_{j}"]
                    )
                    
            except Exception as e:
                print(f"   âŒ {os.path.basename(file_path)}: {e}")
        
        print(f"\n   ğŸ“š ì´ {len(self.files_data)}ê°œ íŒŒì¼ ë¶„ì„ ì™„ë£Œ")
    
    def _analyze_single_document(self, file_id: str, filename: str, content: str) -> Dict:
        """ë‹¨ì¼ ë¬¸ì„œ ë¶„ì„"""
        
        # ë¬¸ì„œ ìœ í˜• ì¶”ë¡ 
        doc_type = self._infer_doc_type(filename, content)
        
        # ë‚ ì§œ ì¶”ì¶œ
        date = self._extract_date(content)
        
        # ê¸ˆì•¡ ì¶”ì¶œ
        amount = self._extract_amount(content)
        
        # ê´€ë ¨ ì—…ì²´/ë‹¹ì‚¬ì ì¶”ì¶œ
        parties = self._extract_parties(content)
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._extract_keywords(content)
        
        # ìš”ì•½ ìƒì„± (contentëŠ” í¬í•¨í•˜ì§€ ì•ŠìŒ - í”„ë¡ íŠ¸ì—”ë“œ ìŠ¤í™)
        summary = self._generate_summary(content[:2000])
        
        return {
            "id": file_id,
            "name": filename,
            "docType": doc_type,
            "date": date,
            "amount": amount,
            "parties": parties,
            "summary": summary,
            "keywords": keywords
        }
    
    def _infer_doc_type(self, filename: str, content: str) -> str:
        """ë¬¸ì„œ ìœ í˜• ì¶”ë¡ """
        name_lower = filename.lower()
        content_lower = content[:500].lower()
        
        type_keywords = {
            "ê¸°ì•ˆ": ["ê¸°ì•ˆ", "í’ˆì˜"],
            "ê³„íš": ["ê³„íš", "ê¸°ë³¸ê³„íš", "ìˆ˜ë¦½"],
            "ê³„ì•½": ["ê³„ì•½ì„œ", "ê³„ì•½", "ìš©ì—­"],
            "ê²€ìˆ˜": ["ê²€ìˆ˜", "ë‚©í’ˆ", "ê²€ì‚¬"],
            "ì •ì‚°": ["ì •ì‚°", "ê²°ì‚°", "ì¢…ë£Œ"]
        }
        
        for doc_type, keywords in type_keywords.items():
            if any(kw in name_lower or kw in content_lower for kw in keywords):
                return doc_type
        
        return "ì¼ë°˜"
    
    def _extract_date(self, content: str) -> str:
        """ë‚ ì§œ ì¶”ì¶œ"""
        patterns = [
            r'(\d{4})[-./ë…„]\s*(\d{1,2})[-./ì›”]\s*(\d{1,2})',
            r'(\d{4})(\d{2})(\d{2})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                y, m, d = match.groups()
                return f"{y}-{m.zfill(2)}-{d.zfill(2)}"
        
        return ""
    
    def _extract_amount(self, content: str) -> Optional[int]:
        """ê¸ˆì•¡ ì¶”ì¶œ"""
        # ì²œ ë‹¨ìœ„ ì‰¼í‘œ í¬í•¨ ê¸ˆì•¡ íŒ¨í„´
        patterns = [
            r'(\d{1,3}(?:,\d{3})+)\s*ì›',
            r'ê¸ˆ\s*(\d{1,3}(?:,\d{3})+)\s*ì›',
            r'ì´\s*(\d{1,3}(?:,\d{3})+)\s*ì›'
        ]
        
        amounts = []
        for pattern in patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                try:
                    amount = int(match.replace(',', ''))
                    if amount >= 10000:  # 1ë§Œì› ì´ìƒë§Œ
                        amounts.append(amount)
                except:
                    pass
        
        return max(amounts) if amounts else None
    
    def _extract_parties(self, content: str) -> List[str]:
        """ê´€ë ¨ ì—…ì²´/ë‹¹ì‚¬ì ì¶”ì¶œ"""
        patterns = [
            r'\(ì£¼\)\s*([ê°€-í£a-zA-Z]+)',
            r'ì£¼ì‹íšŒì‚¬\s*([ê°€-í£a-zA-Z]+)',
            r'([ê°€-í£]+)\s*(?:ì£¼ì‹íšŒì‚¬|ãˆœ)'
        ]
        
        parties = set()
        for pattern in patterns:
            matches = re.findall(pattern, content)
            parties.update(matches)
        
        return list(parties)[:5]  # ìµœëŒ€ 5ê°œ
    
    def _extract_keywords(self, content: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨ ë²„ì „)"""
        # ìì£¼ ë“±ì¥í•˜ëŠ” ëª…ì‚¬ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
        content = re.sub(r'[^\w\s]', ' ', content)
        words = content.split()
        
        # 2ê¸€ì ì´ìƒ í•œê¸€ ë‹¨ì–´
        korean_words = [w for w in words if re.match(r'^[ê°€-í£]{2,}$', w)]
        
        # ë¹ˆë„ ê³„ì‚°
        from collections import Counter
        word_counts = Counter(korean_words)
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {'ìˆëŠ”', 'í•˜ëŠ”', 'ë˜ëŠ”', 'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜ëŠ”'}
        keywords = [word for word, _ in word_counts.most_common(20) 
                   if word not in stopwords and len(word) >= 2]
        
        return keywords[:10]
    
    def _generate_summary(self, content: str) -> str:
        """AI ìš”ì•½ ìƒì„±"""
        try:
            response = self.client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": "ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ 1-2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”. ê¸ˆì•¡ì´ ìˆìœ¼ë©´ í¬í•¨í•˜ì„¸ìš”."},
                    {"role": "user", "content": content}
                ],
                max_tokens=100,
                temperature=0.0
            )
            return response.choices[0].message.content.strip()
        except:
            return content[:100] + "..."
    
    def _build_project_structure(self) -> Dict:
        """í”„ë¡ íŠ¸ì—”ë“œ ìŠ¤í™ì— ë§ëŠ” êµ¬ì¡° ìƒì„±"""

        # 1. íŒŒì¼ì„ í´ë” êµ¬ì¡°ë¡œ ê·¸ë£¹í™”
        folders = self._group_files_by_phase()

        # 2. íƒ€ì„ë¼ì¸ ìƒì„±
        timeline = self._build_timeline()

        # 3. ì´ìŠˆ ë¶„ì„
        issues = self._analyze_issues()

        # 4. í”„ë¡œì íŠ¸ëª… ì¶”ë¡ 
        project_name = self._infer_project_name()

        # 5. overview ìƒì„±
        overview = self._build_overview(project_name)

        # 6. ì˜ì‚¬ê²°ì • ì¶”ì¶œ
        decisions = self._extract_decisions()

        # 7. ê°€ì´ë“œë¼ì¸ ìƒì„±
        guidelines = self._build_guidelines()

        # 8. ì£¼ìš” ë¬¸ì„œ ì„ ì •
        key_files = self._select_key_files()

        # ìµœì¢… êµ¬ì¡° (í”„ë¡ íŠ¸ì—”ë“œ ì „ì²´ ìŠ¤í™ ì¤€ìˆ˜)
        return {
            "id": f"proj-{datetime.now().strftime('%Y%m%d%H%M%S')}",
            "name": project_name,
            "fileCount": len(self.files_data),
            "files": folders,  # íŠ¸ë¦¬ êµ¬ì¡°
            "summary": {
                "overview": overview,
                "timeline": timeline,
                "decisions": decisions,
                "issues": issues,
                "guidelines": guidelines,
                "keyFiles": key_files,
                "totalAmount": sum(f.get("amount") or 0 for f in self.files_data),
                "dateRange": self._get_date_range()
            }
        }
    
    def _group_files_by_phase(self) -> List[Dict]:
        """íŒŒì¼ì„ í˜ì´ì¦ˆë³„ í´ë” êµ¬ì¡°ë¡œ ê·¸ë£¹í™”"""
        phase_folders = {
            "plan": {"name": "01_ê¸°íš", "children": []},
            "contract": {"name": "02_ê³„ì•½", "children": []},
            "execute": {"name": "03_ì§‘í–‰", "children": []},
            "close": {"name": "04_ì •ì‚°", "children": []},
            "etc": {"name": "05_ê¸°íƒ€", "children": []}
        }
        
        for file in self.files_data:
            doc_type = file.get("docType", "ì¼ë°˜")
            
            # ë¬¸ì„œ ìœ í˜•ì—ì„œ í˜ì´ì¦ˆ ê²°ì •
            phase = "etc"
            for keyword, phase_id in self.DOC_TYPE_TO_PHASE.items():
                if keyword in doc_type:
                    phase = phase_id
                    break
            
            # content ì œì™¸ (í”„ë¡ íŠ¸ì—”ë“œ ìŠ¤í™)
            file_info = {k: v for k, v in file.items() if k != "content"}
            phase_folders[phase]["children"].append(file_info)
        
        # ë¹ˆ í´ë” ì œì™¸í•˜ê³  ë°˜í™˜
        return [folder for folder in phase_folders.values() if folder["children"]]
    
    def _build_timeline(self) -> Dict:
        """íƒ€ì„ë¼ì¸ ìƒì„±"""
        phases = [
            {"id": "plan", "name": "ê¸°íš", "color": "#3b82f6"},
            {"id": "contract", "name": "ê³„ì•½", "color": "#8b5cf6"},
            {"id": "execute", "name": "ì§‘í–‰", "color": "#10b981"},
            {"id": "close", "name": "ì •ì‚°", "color": "#f59e0b"},
            {"id": "etc", "name": "ê¸°íƒ€", "color": "#6b7280"},
        ]

        events = []
        for file in self.files_data:
            doc_type = file.get("docType", "ì¼ë°˜")

            # í˜ì´ì¦ˆ ê²°ì •
            phase_id = "etc"
            for keyword, pid in self.DOC_TYPE_TO_PHASE.items():
                if keyword in doc_type:
                    phase_id = pid
                    break

            # í•˜ì´ë¼ì´íŠ¸ ê²°ì • (ì¤‘ìš” ì´ë²¤íŠ¸)
            highlight = any(kw in file.get("name", "") for kw in ["ë³€ê²½", "ì¶”ê°€", "ì •ì •"])

            # ë¼ë²¨: íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°, ë²ˆí˜¸ ì ‘ë‘ì‚¬ ì •ë¦¬
            raw_name = os.path.splitext(file["name"])[0]
            # "01_ê¸°ë³¸ê³„íšìˆ˜ë¦½(ê¸°ì•ˆ)" â†’ "ê¸°ë³¸ê³„íšìˆ˜ë¦½(ê¸°ì•ˆ)" (ì•ì˜ ë²ˆí˜¸_ ì œê±°)
            label = re.sub(r'^\d+[_.\-]\s*', '', raw_name)
            if not label:
                label = raw_name

            # ë‚ ì§œê°€ ì—†ëŠ” íŒŒì¼ë„ ì´ë²¤íŠ¸ì— í¬í•¨ (ë‚ ì§œ ì—†ìŒ í‘œì‹œ)
            event_date = file.get("date", "")

            events.append({
                "date": event_date,
                "label": label,
                "description": file.get("summary", "")[:100],
                "phaseId": phase_id,
                "fileId": file["id"],
                "amount": file.get("amount"),
                "highlight": highlight,
            })

        # ë‚ ì§œ ìˆëŠ” ê²ƒ ë¨¼ì €, ë‚ ì§œìˆœ ì •ë ¬. ë‚ ì§œ ì—†ëŠ” ê²ƒì€ ë’¤ì—
        dated = [e for e in events if e["date"]]
        undated = [e for e in events if not e["date"]]
        dated.sort(key=lambda x: x["date"])

        return {"phases": phases, "events": dated + undated}
    
    def _build_overview(self, project_name: str) -> Dict:
        """í”„ë¡œì íŠ¸ ê°œìš” ìƒì„± (í”„ë¡ íŠ¸ì—”ë“œ overview ìŠ¤í™)"""
        date_range = self._get_date_range()
        total_amount = sum(f.get("amount") or 0 for f in self.files_data)

        # LLMìœ¼ë¡œ ì„¤ëª… ìƒì„±
        description = ""
        try:
            file_list = "\n".join(
                f"- {f['name']} ({f.get('docType', 'ì¼ë°˜')}): {f.get('summary', '')[:60]}"
                for f in self.files_data[:10]
            )
            response = self.client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": "ë¬¸ì„œ ëª©ë¡ì„ ë³´ê³  ì´ ì—…ë¬´/í”„ë¡œì íŠ¸ë¥¼ 2~3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µí•˜ì„¸ìš”."},
                    {"role": "user", "content": f"í”„ë¡œì íŠ¸: {project_name}\n\në¬¸ì„œ ëª©ë¡:\n{file_list}"}
                ],
                max_tokens=150,
                temperature=0.0
            )
            description = response.choices[0].message.content.strip()
        except Exception:
            description = f"{len(self.files_data)}ê°œ ë¬¸ì„œë¡œ êµ¬ì„±ëœ ì—…ë¬´ì…ë‹ˆë‹¤."

        period = ""
        if date_range.get("start") and date_range.get("end"):
            period = f"{date_range['start']} ~ {date_range['end']}"
        elif date_range.get("start"):
            period = date_range["start"]

        return {
            "title": project_name,
            "description": description,
            "period": period,
            "budget": total_amount,
            "status": "unknown",
        }

    def _extract_decisions(self) -> List[Dict]:
        """ì£¼ìš” ì˜ì‚¬ê²°ì • ì¶”ì¶œ (ë³€ê²½, ì¶”ê°€, ì •ì • ë“±)"""
        decisions = []
        for file in self.files_data:
            name = file.get("name", "")
            if any(kw in name for kw in ["ë³€ê²½", "ì¶”ê°€", "ì •ì •", "ìˆ˜ì •", "ì¦ì•¡", "ê°ì•¡"]):
                decisions.append({
                    "date": file.get("date", ""),
                    "title": os.path.splitext(name)[0],
                    "description": file.get("summary", "")[:150],
                    "impact": f"{file['amount']:,}ì›" if file.get("amount") else "",
                    "relatedFileIds": [file["id"]],
                })
        return decisions

    def _build_guidelines(self) -> List[Dict]:
        """ê°€ì´ë“œë¼ì¸ ìƒì„±"""
        items = []

        # ì´ìŠˆ ê¸°ë°˜ ê°€ì´ë“œë¼ì¸
        doc_types = [f.get("docType", "") for f in self.files_data]
        if "ê³„ì•½" in doc_types:
            items.append("ê³„ì•½ ê´€ë ¨ ë¬¸ì„œì˜ ìœ íš¨ê¸°ê°„ ë° ë³€ê²½ì‚¬í•­ì„ í™•ì¸í•˜ì„¸ìš”.")
        if any("ë³€ê²½" in name for name in [f.get("name", "") for f in self.files_data]):
            items.append("ë³€ê²½ ë¬¸ì„œê°€ ìˆìŠµë‹ˆë‹¤. ë³€ê²½ ì‚¬ìœ ì™€ ìŠ¹ì¸ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

        dates_missing = sum(1 for f in self.files_data if not f.get("date"))
        if dates_missing > 0:
            items.append(f"{dates_missing}ê°œ ë¬¸ì„œì—ì„œ ë‚ ì§œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ í™•ì¸í•˜ì„¸ìš”.")

        if not items:
            items.append("ê° ë¬¸ì„œì˜ ì‘ì„±ì¼ê³¼ ê¸ˆì•¡ì„ êµì°¨ í™•ì¸í•˜ì„¸ìš”.")
            items.append("ëˆ„ë½ëœ í›„ì† ë¬¸ì„œê°€ ìˆëŠ”ì§€ ì ê²€í•˜ì„¸ìš”.")

        return [{"title": "í›„ì† ì—…ë¬´", "items": items}]

    def _select_key_files(self) -> List[Dict]:
        """ì£¼ìš” ë¬¸ì„œ ì„ ì •"""
        key_files = []

        # ê¸ˆì•¡ì´ í° ìˆœìœ¼ë¡œ ìƒìœ„ 3ê°œ
        files_with_amount = [f for f in self.files_data if f.get("amount")]
        files_with_amount.sort(key=lambda x: x["amount"], reverse=True)

        for f in files_with_amount[:3]:
            reason = f"ê¸ˆì•¡ {f['amount']:,}ì› â€” {f.get('docType', 'ë¬¸ì„œ')}"
            key_files.append({"fileId": f["id"], "reason": reason})

        # ê¸ˆì•¡ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ íŒŒì¼ì´ë¼ë„
        if not key_files and self.files_data:
            f = self.files_data[0]
            key_files.append({"fileId": f["id"], "reason": "ì²« ë²ˆì§¸ ë¬¸ì„œ"})

        return key_files

    def _analyze_issues(self) -> List[Dict]:
        """ì´ìŠˆ ìë™ ë¶„ì„"""
        issues = []
        
        for file in self.files_data:
            name = file.get("name", "")
            doc_type = file.get("docType", "")
            
            # ë³€ê²½ê³„ì•½ ê´€ë ¨
            if "ë³€ê²½" in name or "ìˆ˜ì •" in name:
                issues.append({
                    "level": "warn",
                    "title": "ë³€ê²½ì‚¬í•­ ê°ì§€",
                    "description": f"{name}ì—ì„œ ë³€ê²½ ê´€ë ¨ ë‚´ìš©ì´ í™•ì¸ë©ë‹ˆë‹¤.",
                    "suggestion": "ë³€ê²½ ì‚¬ìœ  ë° ìŠ¹ì¸ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                    "relatedFileIds": [file["id"]]
                })
            
            # ê¸ˆì•¡ ê´€ë ¨ (ê³ ì•¡)
            amount = file.get("amount")
            if amount and amount >= 100000000:  # 1ì–µ ì´ìƒ
                issues.append({
                    "level": "info",
                    "title": "ê³ ì•¡ ê±°ë˜ í™•ì¸",
                    "description": f"{name}ì—ì„œ {amount:,}ì› ê·œëª¨ì˜ ê¸ˆì•¡ì´ í™•ì¸ë©ë‹ˆë‹¤.",
                    "suggestion": "ê²°ì¬ ê¶Œí•œ ë° ê³„ì•½ ë°©ì‹ì„ ê²€í† í•˜ì„¸ìš”.",
                    "relatedFileIds": [file["id"]]
                })
            
            # ë‚ ì§œ ëˆ„ë½
            if not file.get("date"):
                issues.append({
                    "level": "warn",
                    "title": "ì¼ì ì •ë³´ ëˆ„ë½",
                    "description": f"{name}ì—ì„œ ë‚ ì§œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "suggestion": "ë¬¸ì„œì˜ ì‘ì„±ì¼ ë˜ëŠ” ì‹œí–‰ì¼ì„ í™•ì¸í•˜ì„¸ìš”.",
                    "relatedFileIds": [file["id"]]
                })
        
        return issues
    
    def _infer_project_name(self) -> str:
        """í”„ë¡œì íŠ¸ëª… ì¶”ë¡ """
        # ì²« ë²ˆì§¸ íŒŒì¼ì˜ í‚¤ì›Œë“œì—ì„œ ì¶”ë¡ 
        if self.files_data:
            keywords = self.files_data[0].get("keywords", [])
            if keywords:
                return " ".join(keywords[:3]) + " ì¸ìˆ˜ì¸ê³„"
        
        return f"ì¸ìˆ˜ì¸ê³„ í”„ë¡œì íŠ¸ {datetime.now().strftime('%Y%m%d')}"
    
    def _get_date_range(self) -> Dict:
        """ë‚ ì§œ ë²”ìœ„ ê³„ì‚°"""
        dates = [f["date"] for f in self.files_data if f.get("date")]
        
        if not dates:
            return {"start": "", "end": ""}
        
        return {
            "start": min(dates),
            "end": max(dates)
        }
    
    def query(self, question: str) -> Dict:
        """ì§ˆë¬¸ ë‹µë³€ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)"""
        # ChromaDB ê²€ìƒ‰
        results = self.collection.query(
            query_texts=[question],
            n_results=5
        )
        
        if not results['documents'][0]:
            return {"answer": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        context = "\n\n".join(results['documents'][0])
        
        # AI ì‘ë‹µ
        try:
            response = self.client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": f"ì¸ìˆ˜ì¸ê³„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n{PUBLIC_INSTITUTION_GUIDELINES}"},
                    {"role": "user", "content": f"ì°¸ê³ :\n{context}\n\nì§ˆë¬¸: {question}"}
                ],
                temperature=0.0
            )
            
            return {
                "question": question,
                "answer": response.choices[0].message.content,
                "sources": [m.get("source") for m in results['metadatas'][0]]
            }
        except Exception as e:
            return {"error": str(e)}


def main():
    print("\nğŸš€ ë¬¸ì„œ ìë™ ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘...\n")
    
    analyzer = DocumentAnalyzer()
    
    if not analyzer.setup():
        return
    
    # ìë™ ë¶„ì„ ì‹¤í–‰
    print("\n" + "=" * 70)
    print("ğŸ“Š ë¬¸ì„œ ìë™ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("=" * 70)
    
    result = analyzer.analyze_all()
    
    if result:
        print("\n" + "=" * 70)
        print("ğŸ“‹ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("=" * 70)
        print(f"   í”„ë¡œì íŠ¸: {result.get('name', 'N/A')}")
        print(f"   íŒŒì¼ ìˆ˜: {result.get('fileCount', 0)}ê°œ")
        print(f"   ì´ìŠˆ: {len(result.get('summary', {}).get('issues', []))}ê°œ")
        
        if result.get('summary', {}).get('totalAmount'):
            print(f"   ì´ ê¸ˆì•¡: {result['summary']['totalAmount']:,}ì›")
        
        date_range = result.get('summary', {}).get('dateRange', {})
        if date_range.get('start'):
            print(f"   ê¸°ê°„: {date_range['start']} ~ {date_range['end']}")
        
        print("=" * 70)
        
        print("\nâœ… JSON ë¶„ì„ ì™„ë£Œ!")
        print("\nğŸ’¡ ì±—ë´‡ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´:")
        print("   python handover_rag_v3.py")


if __name__ == "__main__":
    main()
